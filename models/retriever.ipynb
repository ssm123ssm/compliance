{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "loader = PyPDFLoader(\"data/act.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNKS_OVERLAP = 200\n",
    "TOP_K = 5\n",
    "MODEL_NAME = \"gpt-3.5-turbo\"\n",
    "EMBEDDING_MODEL = \"text-embedding-3-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "import anthropic\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = CHUNK_SIZE, chunk_overlap = CHUNKS_OVERLAP)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(docs, OpenAIEmbeddings(model=EMBEDDING_MODEL))\n",
    "retriever = vector.as_retriever(search_kwargs={\"k\": TOP_K})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_related_docs(query):\n",
    "    related_docs = retriever.get_relevant_documents(query)\n",
    "    return related_docs\n",
    "\n",
    "def get_page_content(related_docs):\n",
    "    page_content = \"\"\n",
    "    for i, doc in enumerate(related_docs):\n",
    "        page_content += f\"START OF CONTEXT PART {i+1}\\n\\n{doc.page_content}\\n\\nEND OF CONTEXT PART {i+1}\\n\\n\"\n",
    "    return page_content\n",
    "\n",
    "def create_query(query):\n",
    "    related_docs = get_related_docs(query)\n",
    "    page_content = get_page_content(related_docs)\n",
    "    final_query = f\"\"\"\n",
    "    You are an expert banker with expert knowledge on complience and regulations. This question is asked from you.\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    You are given relevent sections from the legal document as context. You need to provide the answer to the question based on the given context only.\n",
    "\n",
    "    Context: {page_content}\n",
    "\n",
    "    Give your answer only based on the context given above. If can't find the answer in the given context, say \"I can't find the answer in the given context\".\n",
    "\n",
    "\"\"\"\n",
    "    return final_query\n",
    "\n",
    "def ask_claud(query):\n",
    "    response = anthropic.Anthropic().messages.create(\n",
    "        model=\"claude-3-sonnet-20240229\",\n",
    "        max_tokens=512,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": create_query(query)}\n",
    "        ]\n",
    "    )\n",
    "    return(response.content[0].text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:3002\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "127.0.0.1 - - [09/Mar/2024 15:29:11] \"GET /status HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Mar/2024 15:29:11] \"GET /status HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When are closure orders issued for a licensed commercial bank incorporated outside Sri Lanka, according to the Banking Act?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [09/Mar/2024 15:30:26] \"POST /chat HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under the Banking Act, which section outlines the liability of the acquiring bank regarding foreign currency depositors of a defaulting bank?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [09/Mar/2024 15:40:35] \"POST /chat HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, jsonify, request, json\n",
    "from flask_cors import CORS\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "@app.route('/status', methods=['GET'])\n",
    "def get_status():\n",
    "    #return status ok\n",
    "    return jsonify({'status': 'ok'}), 200\n",
    "\n",
    "\n",
    "#add a POST route at /chat. Get the request body and echo the message back\n",
    "@app.route('/chat', methods=['POST'])\n",
    "def chat():\n",
    "    data = json.loads(request.data)\n",
    "    \n",
    "    #get the last message from the user\n",
    "    message = data['messages'][-1]['content']\n",
    "    print(message)\n",
    "    response = ask_claud(message)\n",
    "    #message = data['messages']\n",
    "    return jsonify({'status': 'ok', 'response': response}), 200\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(port=3002)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
